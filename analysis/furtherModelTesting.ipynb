{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Exploration of Machine Learning Models\n",
    "As explained in baselineModelTesting.ipynb, we want to predict lap distance given other lap data. We chose to use a Linear Regression algorithm for the baseline model, and got RMSE and MAE values of 1.88 and 0.8 respectively. Now, we will train models on the other candidate algorithms (both with and without cross validation) and compare to this baseline model. These algorithms include KNN, Decision Trees, Random Forests, and Gradient Boosting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules and Clean the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>totalpowerin</th>\n",
       "      <th>totalpowerout</th>\n",
       "      <th>netpowerout</th>\n",
       "      <th>distance</th>\n",
       "      <th>amphours</th>\n",
       "      <th>batterysecondsremaining</th>\n",
       "      <th>averagespeed</th>\n",
       "      <th>averagepackCurrent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>793.532455</td>\n",
       "      <td>1803.385827</td>\n",
       "      <td>1009.853372</td>\n",
       "      <td>2.814824</td>\n",
       "      <td>97.800003</td>\n",
       "      <td>22425</td>\n",
       "      <td>49.584229</td>\n",
       "      <td>15.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>798.704841</td>\n",
       "      <td>685.739611</td>\n",
       "      <td>-112.965230</td>\n",
       "      <td>4.118535</td>\n",
       "      <td>95.300003</td>\n",
       "      <td>56521</td>\n",
       "      <td>24.939703</td>\n",
       "      <td>6.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>585.339547</td>\n",
       "      <td>2634.298876</td>\n",
       "      <td>2048.959329</td>\n",
       "      <td>3.974127</td>\n",
       "      <td>93.599998</td>\n",
       "      <td>14468</td>\n",
       "      <td>54.412831</td>\n",
       "      <td>23.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>681.537550</td>\n",
       "      <td>2573.828806</td>\n",
       "      <td>1892.291255</td>\n",
       "      <td>3.987067</td>\n",
       "      <td>91.900002</td>\n",
       "      <td>14517</td>\n",
       "      <td>55.419749</td>\n",
       "      <td>22.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>559.875099</td>\n",
       "      <td>2739.047764</td>\n",
       "      <td>2179.172665</td>\n",
       "      <td>3.994517</td>\n",
       "      <td>90.099998</td>\n",
       "      <td>13283</td>\n",
       "      <td>55.529298</td>\n",
       "      <td>24.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>537.555507</td>\n",
       "      <td>1679.129062</td>\n",
       "      <td>1141.573555</td>\n",
       "      <td>3.952078</td>\n",
       "      <td>3.800000</td>\n",
       "      <td>740</td>\n",
       "      <td>44.881966</td>\n",
       "      <td>18.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>475.938651</td>\n",
       "      <td>1666.858482</td>\n",
       "      <td>1190.919832</td>\n",
       "      <td>3.963359</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>406</td>\n",
       "      <td>43.894226</td>\n",
       "      <td>18.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>700.176665</td>\n",
       "      <td>566.324558</td>\n",
       "      <td>-133.852107</td>\n",
       "      <td>0.181906</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>100215</td>\n",
       "      <td>1.106129</td>\n",
       "      <td>-5.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>615.194700</td>\n",
       "      <td>1680.524756</td>\n",
       "      <td>1065.330056</td>\n",
       "      <td>3.963687</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>611</td>\n",
       "      <td>41.645220</td>\n",
       "      <td>17.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>624.008188</td>\n",
       "      <td>1543.656620</td>\n",
       "      <td>919.648432</td>\n",
       "      <td>3.966359</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>300</td>\n",
       "      <td>43.276732</td>\n",
       "      <td>16.79</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     totalpowerin  totalpowerout  netpowerout  distance   amphours  \\\n",
       "3      793.532455    1803.385827  1009.853372  2.814824  97.800003   \n",
       "4      798.704841     685.739611  -112.965230  4.118535  95.300003   \n",
       "5      585.339547    2634.298876  2048.959329  3.974127  93.599998   \n",
       "6      681.537550    2573.828806  1892.291255  3.987067  91.900002   \n",
       "7      559.875099    2739.047764  2179.172665  3.994517  90.099998   \n",
       "..            ...            ...          ...       ...        ...   \n",
       "202    537.555507    1679.129062  1141.573555  3.952078   3.800000   \n",
       "203    475.938651    1666.858482  1190.919832  3.963359   2.100000   \n",
       "204    700.176665     566.324558  -133.852107  0.181906   4.700000   \n",
       "205    615.194700    1680.524756  1065.330056  3.963687   3.000000   \n",
       "206    624.008188    1543.656620   919.648432  3.966359   1.400000   \n",
       "\n",
       "     batterysecondsremaining  averagespeed  averagepackCurrent  \n",
       "3                      22425     49.584229               15.70  \n",
       "4                      56521     24.939703                6.07  \n",
       "5                      14468     54.412831               23.29  \n",
       "6                      14517     55.419749               22.79  \n",
       "7                      13283     55.529298               24.42  \n",
       "..                       ...           ...                 ...  \n",
       "202                      740     44.881966               18.49  \n",
       "203                      406     43.894226               18.62  \n",
       "204                   100215      1.106129               -5.78  \n",
       "205                      611     41.645220               17.67  \n",
       "206                      300     43.276732               16.79  \n",
       "\n",
       "[199 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "\n",
    "#global variables for randomness seed and test size\n",
    "global_random_state = 69420\n",
    "global_test_size = 0.2\n",
    "\n",
    "# Drop useless columns\n",
    "packetTrainingDataPath=\"../training_data/Elysia.Laps.feather\"\n",
    "df = pd.read_feather(packetTrainingDataPath)\n",
    "df = df.drop(\n",
    "        columns=[\n",
    "            \"msgType\",\n",
    "            \"_id.$oid\",\n",
    "            \"averagepackCurrent.$numberDouble\",\n",
    "            \"timestamp.$numberLong\",\n",
    "            \"secondsdifference\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "#we need the averagepackCurrent data to be numeric instead of {\"$numberDouble\": \"NaN\"}, setting errors='coerce' sets them to numerical NaN\n",
    "df['averagepackCurrent'] = pd.to_numeric(df['averagepackCurrent'], errors='coerce')\n",
    "\n",
    "#drop the 4 rows with null values\n",
    "df = df.dropna(subset=['distance', 'averagepackCurrent', 'averagespeed'])\n",
    "\n",
    "# Remove outliers from the data\n",
    "#to do this, first define a threshold for outliers (3 standard deviations which contains 99.7% of data)\n",
    "threshold = 3 * np.std(df['distance'])\n",
    "\n",
    "#remove the outliers\n",
    "df = df[(df['distance'] >= -threshold) & (df['distance'] <= threshold)]\n",
    "#remove negative distance values\n",
    "df = df[df['distance'] >= 0]\n",
    "\n",
    "#seperate distance from the other features\n",
    "X = df[['totalpowerin', 'totalpowerout', 'netpowerout', 'amphours', \n",
    "    'averagepackCurrent', 'batterysecondsremaining', 'averagespeed']]\n",
    "y = df['distance']\n",
    "\n",
    "display(df) #should be 199x8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the n_neighbors number the model should be training on: 20\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE for n_neighbors=1: 0.5410108892598948\n",
      "MAE for n_neighbors=2: 0.5426444457645021\n",
      "MAE for n_neighbors=3: 0.5513075877791929\n",
      "MAE for n_neighbors=4: 0.573686912528912\n",
      "MAE for n_neighbors=5: 0.5495526073067393\n",
      "MAE for n_neighbors=6: 0.5634956319442919\n",
      "MAE for n_neighbors=7: 0.5732355218202718\n",
      "MAE for n_neighbors=8: 0.5738835808214869\n",
      "MAE for n_neighbors=9: 0.571293157667966\n",
      "MAE for n_neighbors=10: 0.5607889033389646\n",
      "MAE for n_neighbors=11: 0.5448196712094987\n",
      "MAE for n_neighbors=12: 0.5364807313533111\n",
      "MAE for n_neighbors=13: 0.5280962038800316\n",
      "MAE for n_neighbors=14: 0.5206492283871483\n",
      "MAE for n_neighbors=15: 0.50927465617411\n",
      "MAE for n_neighbors=16: 0.5081548192759396\n",
      "MAE for n_neighbors=17: 0.5028496564392458\n",
      "MAE for n_neighbors=18: 0.4968811868517823\n",
      "MAE for n_neighbors=19: 0.4902695300407429\n",
      "MAE for n_neighbors=20: 0.4865336079257342\n",
      "RMSE for KNN is: 1.7631053829435306\n",
      "MAE for KNN is: 0.9387104717208447\n"
     ]
    }
   ],
   "source": [
    "#HYPERPARAMTER TUNING FOR KNN\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=global_test_size, random_state=global_random_state)\n",
    "\n",
    "#train the model with KNN\n",
    "#knn_model = KNeighborsRegressor(n_neighbors=5)\n",
    "# Define the parameter grid\n",
    "param_grid = {'n_neighbors': range(1, 21)}\n",
    "\n",
    "# Use GridSearchCV to find the best number of neighbors\n",
    "grid_search = GridSearchCV(KNeighborsRegressor(), param_grid, scoring='neg_mean_absolute_error', cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Set the best number of neighbors\n",
    "knn_model = KNeighborsRegressor(n_neighbors=grid_search.best_params_['n_neighbors'])\n",
    "print(f\"This is the n_neighbors number the model should be training on: {grid_search.best_params_['n_neighbors']}\")\n",
    "knn_model.fit(X_train, y_train)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the MAE for each number of n_neighbors\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(param_grid['n_neighbors'], -grid_search.cv_results_['mean_test_score'], marker='o')\n",
    "plt.xlabel('Number of Neighbors (n_neighbors)')\n",
    "plt.ylabel('Mean Absolute Error (MAE)')\n",
    "plt.title('MAE for each number of n_neighbors')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "mae_scores = -grid_search.cv_results_['mean_test_score']\n",
    "for n, mae in zip(param_grid['n_neighbors'], mae_scores):\n",
    "    print(f\"MAE for n_neighbors={n}: {mae}\")\n",
    "\n",
    "#get predictions on the test data\n",
    "y_pred_knn = knn_model.predict(X_test)\n",
    "\n",
    "#evaluate using RMSE and MAE\n",
    "rmse_knn = np.sqrt(mean_squared_error(y_test, y_pred_knn))\n",
    "mae_knn = mean_absolute_error(y_test, y_pred_knn)\n",
    "\n",
    "print(f\"RMSE for KNN is: {rmse_knn}\")\n",
    "print(f\"MAE for KNN is: {mae_knn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Decision Trees is: 1.7601743147139357\n",
      "MAE for Decision Trees is: 0.93758609865129\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=global_test_size, random_state=global_random_state)\n",
    "\n",
    "newKNN_model = KNeighborsRegressor(n_neighbors=5)\n",
    "newKNN_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_dt = newKNN_model.predict(X_test)\n",
    "\n",
    "rmse_newknn = np.sqrt(mean_squared_error(y_test, y_pred_dt))\n",
    "mae_newknn = mean_absolute_error(y_test, y_pred_dt)\n",
    "\n",
    "print(f\"RMSE for Decision Trees is: {rmse_newknn}\")\n",
    "print(f\"MAE for Decision Trees is: {mae_newknn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average RMSE of KNN with Cross Validation is: 1.3027346860967934\n",
      "Average MAE of KNN with Cross Validation is: 0.6564721744528997\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=global_random_state)\n",
    "knn_model = KNeighborsRegressor(n_neighbors=5)\n",
    "\n",
    "rmse_scores_knn = []\n",
    "mae_scores_knn = []\n",
    "\n",
    "for train_index, val_index in kf.split(X):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    knn_model.fit(X_train, y_train)\n",
    "    y_pred_knn = knn_model.predict(X_val)\n",
    "    \n",
    "    rmse_scores_knn.append(np.sqrt(mean_squared_error(y_val, y_pred_knn)))\n",
    "    mae_scores_knn.append(mean_absolute_error(y_val, y_pred_knn))\n",
    "\n",
    "print(f\"Average RMSE of KNN with Cross Validation is: {np.mean(rmse_scores_knn)}\")\n",
    "print(f\"Average MAE of KNN with Cross Validation is: {np.mean(mae_scores_knn)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Decision Trees is: 1.9269982143851523\n",
      "MAE for Decision Trees is: 1.0067465393990278\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=global_test_size, random_state=global_random_state)\n",
    "\n",
    "dt_model = DecisionTreeRegressor(random_state=global_random_state)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "\n",
    "rmse_dt = np.sqrt(mean_squared_error(y_test, y_pred_dt))\n",
    "mae_dt = mean_absolute_error(y_test, y_pred_dt)\n",
    "\n",
    "print(f\"RMSE for Decision Trees is: {rmse_dt}\")\n",
    "print(f\"MAE for Decision Trees is: {mae_dt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average RMSE of Decision Trees with Cross Validation is: 1.6194096052355795\n",
      "Average MAE of Decision Trees with Cross Validation is: 0.7345414044039864\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=global_random_state)\n",
    "dt_model = DecisionTreeRegressor(random_state=global_random_state)\n",
    "\n",
    "rmse_scores_dt = []\n",
    "mae_scores_dt = []\n",
    "\n",
    "for train_index, val_index in kf.split(X):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    dt_model.fit(X_train, y_train)\n",
    "    y_pred_dt = dt_model.predict(X_val)\n",
    "    \n",
    "    rmse_scores_dt.append(np.sqrt(mean_squared_error(y_val, y_pred_dt)))\n",
    "    mae_scores_dt.append(mean_absolute_error(y_val, y_pred_dt))\n",
    "\n",
    "print(f\"Average RMSE of Decision Trees with Cross Validation is: {np.mean(rmse_scores_dt)}\")\n",
    "print(f\"Average MAE of Decision Trees with Cross Validation is: {np.mean(mae_scores_dt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters for Random Forest: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "RMSE for Random Forests is: 1.6678055537819902\n",
      "MAE for Random Forests is: 0.9348572302396694\n"
     ]
    }
   ],
   "source": [
    "# HYPERPARAMETER TUNING FOR RANDOM FORESTS\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=global_test_size, random_state=global_random_state)\n",
    "# Define the parameter grid for Random Forest\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to find the best hyperparameters for Random Forest\n",
    "grid_search_rf = GridSearchCV(RandomForestRegressor(random_state=global_random_state), param_grid_rf, scoring='neg_mean_absolute_error', cv=5)\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Set the best hyperparameters\n",
    "rf_model = RandomForestRegressor(**grid_search_rf.best_params_, random_state=global_random_state)\n",
    "print(f\"Best hyperparameters for Random Forest: {grid_search_rf.best_params_}\")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions on the test data\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate using RMSE and MAE\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"RMSE for Random Forests is: {rmse_rf}\")\n",
    "print(f\"MAE for Random Forests is: {mae_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Random Forests is: 1.7014263662227194\n",
      "MAE for Random Forests is: 0.9749912091165533\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=global_test_size, random_state=global_random_state)\n",
    "\n",
    "rf_model = RandomForestRegressor(random_state=global_random_state)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"RMSE for Random Forests is: {rmse_rf}\")\n",
    "print(f\"MAE for Random Forests is: {mae_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average RMSE of Random Forests with Cross Validation is: 1.1449132726496747\n",
      "Average MAE of Random Forests with Cross Validation is: 0.6261941042077885\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=global_random_state)\n",
    "rf_model = RandomForestRegressor(random_state=global_random_state)\n",
    "\n",
    "rmse_scores_rf = []\n",
    "mae_scores_rf = []\n",
    "\n",
    "for train_index, val_index in kf.split(X):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    rf_model.fit(X_train, y_train)\n",
    "    y_pred_rf = rf_model.predict(X_val)\n",
    "    \n",
    "    rmse_scores_rf.append(np.sqrt(mean_squared_error(y_val, y_pred_rf)))\n",
    "    mae_scores_rf.append(mean_absolute_error(y_val, y_pred_rf))\n",
    "\n",
    "print(f\"Average RMSE of Random Forests with Cross Validation is: {np.mean(rmse_scores_rf)}\")\n",
    "print(f\"Average MAE of Random Forests with Cross Validation is: {np.mean(mae_scores_rf)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for Gradient Boosting is: 1.6968263362694784\n",
      "MAE for Gradient Boosting is: 0.9372560536881501\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=global_test_size, random_state=global_random_state)\n",
    "\n",
    "gb_model = GradientBoostingRegressor(random_state=global_random_state)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "\n",
    "rmse_gb = np.sqrt(mean_squared_error(y_test, y_pred_gb))\n",
    "mae_gb = mean_absolute_error(y_test, y_pred_gb)\n",
    "\n",
    "print(f\"RMSE for Gradient Boosting is: {rmse_gb}\")\n",
    "print(f\"MAE for Gradient Boosting is: {mae_gb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average RMSE of Gradient Boosting with Cross Validation is: 1.2157971010564363\n",
      "Average MAE of Gradient Boosting with Cross Validation is: 0.6490862023426779\n",
      "Minimum distance: 0.0162265625\n",
      "Maximum distance: 8.745853515625\n",
      "Average distance: 4.063722782813743\n",
      "25th percentile of distance: 3.963570068359375\n",
      "50th percentile (median) of distance: 3.9834296875\n",
      "75th percentile of distance: 4.0039501953125\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=global_random_state)\n",
    "gb_model = GradientBoostingRegressor(random_state=global_random_state)\n",
    "\n",
    "rmse_scores_gb = []\n",
    "mae_scores_gb = []\n",
    "\n",
    "for train_index, val_index in kf.split(X):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    gb_model.fit(X_train, y_train)\n",
    "    y_pred_gb = gb_model.predict(X_val)\n",
    "    \n",
    "    rmse_scores_gb.append(np.sqrt(mean_squared_error(y_val, y_pred_gb)))\n",
    "    mae_scores_gb.append(mean_absolute_error(y_val, y_pred_gb))\n",
    "\n",
    "print(f\"Average RMSE of Gradient Boosting with Cross Validation is: {np.mean(rmse_scores_gb)}\")\n",
    "print(f\"Average MAE of Gradient Boosting with Cross Validation is: {np.mean(mae_scores_gb)}\")\n",
    "\n",
    "min_distance = df['distance'].min()\n",
    "max_distance = df['distance'].max()\n",
    "average_distance = df['distance'].mean()\n",
    "percentile_25 = np.percentile(df['distance'], 25)\n",
    "percentile_50 = np.percentile(df['distance'], 50)\n",
    "percentile_75 = np.percentile(df['distance'], 75)\n",
    "\n",
    "print(\"Minimum distance:\", min_distance)\n",
    "print(\"Maximum distance:\", max_distance)\n",
    "print(\"Average distance:\", average_distance)\n",
    "print(f\"25th percentile of distance: {percentile_25}\")\n",
    "print(f\"50th percentile (median) of distance: {percentile_50}\")\n",
    "print(f\"75th percentile of distance: {percentile_75}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking and Evaluation on Performance\n",
    "\n",
    "### Ranking Criteria\n",
    "As mentioned in the conclusion of baselineModelTesting, we want low MAE and RMSE scores, with a priority on lower MAE over RMSE. \n",
    "\n",
    "### Model Performance\n",
    "1. **Random Forests with Cross Validation**\n",
    "    - **MAE:** 0.6262\n",
    "    - **RMSE:** 1.1449\n",
    "\n",
    "2. **Gradient Boosting with Cross Validation**\n",
    "    - **MAE:** 0.6491\n",
    "    - **RMSE:** 1.2158\n",
    "\n",
    "3. **KNN with Cross Validation**\n",
    "    - **MAE:** 0.6565\n",
    "    - **RMSE:** 1.3027\n",
    "\n",
    "4. **Decision Trees with Cross Validation**\n",
    "    - **MAE:** 0.7345\n",
    "    - **RMSE:** 1.6194\n",
    "\n",
    "5. **Random Forests**\n",
    "    - **MAE:** 0.9349\n",
    "    - **RMSE:** 1.6678\n",
    "\n",
    "6. **Gradient Boosting**\n",
    "    - **MAE:** 0.9373\n",
    "    - **RMSE:** 1.6968\n",
    "\n",
    "7. **KNN**\n",
    "    - **MAE:** 0.9376\n",
    "    - **RMSE:** 1.7601\n",
    "\n",
    "8. **Decision Trees**\n",
    "    - **MAE:** 1.0067\n",
    "    - **RMSE:** 1.9270\n",
    "\n",
    "### Summary\n",
    "Based on the MAE scores, the models are ranked as follows:\n",
    "1. Random Forests with Cross Validation\n",
    "2. Gradient Boosting with Cross Validation\n",
    "3. KNN with Cross Validation\n",
    "4. Decision Trees with Cross Validation\n",
    "5. Random Forests\n",
    "6. Gradient Boosting\n",
    "7. KNN\n",
    "8. Decision Trees\n",
    "\n",
    "All these models perform worse than the baseline model, which had MAE and RMSE of 0.57 and 1.07 respectively. Of the new models, the best performing was Random Forests with Cross Validation which has MAE and RMSE scores of 0.6262 and 1.1449. Because the majority of our data is between 3.9-4.0 and MAE is 0.6262, it means predictions are normally 15.66% to 16% off from the real distance value. This is a slight deterioration from the baseline model, which had predictions which were 14.35% different. \n",
    "\n",
    "To be sure the baseline model is truly the best, we will try to improve the top few models through hyperparameter tuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "Lets try to improve the top contending model by tuning hyperparameters with a Grid Search. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning of Random Forests with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=None, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.3s[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   0.4s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.1s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=1, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.3s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.2s\n",
      "[CV] END max_depth=10, min_samples_leaf=2, min_samples_split=5, n_estimators=200; total time=   0.2s\n",
      "Best Parameters: {'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best RMSE Score: 1.1251\n",
      "Best MAE Score: 0.6175\n",
      "Final Average RMSE (Best Model): 1.1251\n",
      "Final Average MAE (Best Model): 0.6175\n"
     ]
    }
   ],
   "source": [
    "#making custom scorers\n",
    "rmse_scorer = make_scorer(mean_squared_error, greater_is_better=False, squared=False)\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "#hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "\n",
    "rf_model = RandomForestRegressor(random_state=global_random_state)\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=global_random_state)\n",
    "\n",
    "#perform GridSearchCV with RMSE and MAE scoring\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid=param_grid,\n",
    "    scoring={'RMSE': rmse_scorer, 'MAE': mae_scorer},\n",
    "    refit='RMSE',\n",
    "    cv=kf,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "#store the best model and its parameters\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best RMSE Score: {-grid_search.best_score_:.4f}\")\n",
    "print(f\"Best MAE Score: {-grid_search.cv_results_['mean_test_MAE'][grid_search.best_index_]:.4f}\")\n",
    "\n",
    "rmse_scores = []\n",
    "mae_scores = []\n",
    "\n",
    "for train_index, val_index in kf.split(X):\n",
    "    X_train, X_val = X.iloc[train_index], X.iloc[val_index]\n",
    "    y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n",
    "    \n",
    "    best_model.fit(X_train, y_train)\n",
    "    y_pred = best_model.predict(X_val)\n",
    "    \n",
    "    rmse_scores.append(np.sqrt(mean_squared_error(y_val, y_pred)))\n",
    "    mae_scores.append(mean_absolute_error(y_val, y_pred))\n",
    "\n",
    "print(f\"Final Average RMSE for Random Forests w/ Cross Validation (on the best model): {np.mean(rmse_scores):.4f}\")\n",
    "print(f\"Final Average MAE for Random Forests w/ Cross Validation (on the best model): {np.mean(mae_scores):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "It is clear to see that hyperparameter tuning did not significantly improve the Random Forest model. Although the RMSE improved from 1.1449 to 1.1251, and MAE from 0.6262 to 0.6175, the baseline Linear Regresison model performs better according to both metrics. \n",
    "\n",
    "### Why the Linear Regression may be outperforming\n",
    "Although one may have expected a more refined and complex algorithm such as Random Forests to perform the best, the baseline regression model may be outperforming more complex algorithms due to the size of our data. with 208 packets (rows) of data, it means only about 160 are being trained on, and 40 to test with after the train_test_split. As explained at the beginning of baselineModelTesting.ipynb, more complex algorithms are prone to overfit on small datasets, which is why we chose a simple Linear Regression as the baseline model. \n",
    "\n",
    "In addition, Linear Regressions generally tend to perform particularly well when relationships within data are easily interpretable or show a linear relation. For example, it should be obvious that the more battery in the car, the more distance it can travel. Or the faster the car travels, the less distance it will be able to travel. Since the lap data is full of simple relationships like these, it will overfit less often than a Random Forest might. \n",
    "\n",
    "### What even is RMSE/MAE? How does it relate to accuracy and precision?\n",
    "MAE (Mean Absolute Error) is the average of the absolute differences between predicted and actual distance values. That is, it measures the average magnitude of errors in a set of predictions, without considering their direction. MAE differs from RMSE in how it is less sensitive to outliers compared to RMSE because it does not square the errors.\n",
    "\n",
    "RMSE (Root Mean Squared Error) is the square root of the average of the squared differences between predicted and actual values. It measures the average magnitude of errors, giving more weight to larger errors due to the squaring it does. That is, RMSE is more sensitive to outliers because it squares the errors, which can disproportionately affect the metric if there are large errors.\n",
    "\n",
    "Here are some examples to put the two simply.\\\n",
    "**MAE**: Imagine you are trying to guess (make a prediction on) the distance a car will travel in a lap. If you guess wrong, MAE tells you, on average, how far off your guesses are from the actual distances. Itâ€™s like saying, \"On average, my guesses are off by about 0.57 units.\"\n",
    "\n",
    "**RMSE**: Now, if you make a few really bad guesses, RMSE will highlight these more because it squares the errors. Itâ€™s like saying, \"If I make a big mistake, it will really show up in my RMSE score.\" RMSE gives you a sense of how bad your worst mistakes are.\n",
    "\n",
    "When it comes to accuracy and precision, accuracy refers to how close the model's predictions are to the actual values. RMSE can be seen as a measure of accuracy because it penalizes larger errors more, thus providing a sense of how far off the predictions can be. Precision refers to the consistency of the model's predictions. A model with high precision will have predictions that are close to each other.\n",
    "MAE can be seen as a measure of precision because it shows the average error without emphasizing larger errors.\n",
    "\n",
    "### What does this mean for Helios?\n",
    "When it comes to Helios, predicting lap distances accurately is useful in optimizing performance and driving strategy. Here are a few scenarios where MAE and RMSE come into play: \n",
    "\n",
    "**Energy Management**:\\\n",
    "MAE: With a low MAE, it means our model is generally good at predicting lap distances. We can use this to plan energy consumption more precisely, ensuring that the car has enough power to complete the race without running out of battery.\n",
    "\n",
    "RMSE: If the RMSE is high, it indicates that there are some laps where the prediction is significantly off. This could mean unexpected energy usage, whether because there is something wrong with the car or due to irregular driving style. So, we may need to have contingency plans for such laps.\n",
    "\n",
    "**Performance Monitoring**:\\\n",
    "MAE: Regularly monitoring MAE will help ensure the model remains reliable over time. If MAE starts increasing, it might indicate that the model needs retraining or that there are changes in the carâ€™s performance.\n",
    "\n",
    "RMSE: Keeping an eye on RMSE will help us catch significant issues early. If RMSE spikes, there was likely outlier data introduced, which could mean thereâ€™s a problem with the car or the data, prompting further investigation into driving behaviour. \n",
    "\n",
    "### Real world example\n",
    "Suppose the model predicts that the car will travel 4.0 units in the next lap, but the actual distance varies. With a MAE of 0.57, we can expect that, on average, the prediction will be within 0.57 units of the actual distance. This helps to plan energy usage more accurately.\n",
    "\n",
    "However, since RMSE is 1.07, it means that while most predictions are close, some could be off by more than 1 unit. This indicates that there should be a buffer in the energy planning to account for these large errors, ensuring that the car doesnâ€™t run out of power unexpectedly.\n",
    "\n",
    "By understanding and applying MAE and RMSE, we can make informed decisions to optimize the carâ€™s performance, manage energy efficiently, and improve overall race strategy.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
