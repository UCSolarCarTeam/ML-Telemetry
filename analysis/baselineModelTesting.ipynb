{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choice and Testing of a Baseline Machine Learning Model #\n",
    "Using the lap data, we want to predict the distance given the other data. To do this, we require regression (prediction) algorithms as opposed to classification models. \n",
    "\n",
    "## Choice of a Baseline Model ##\n",
    "For a baseline model, we want to use a popular yet simple machine learning algorithm. This includes Linear Regression, KNN, Decision Trees, Random Forests, and Gradient Boosting. \n",
    "\n",
    "### **Linear Regression** ###\n",
    "Linear Regression models the relationship between one or more input variables and a continuous output variable by fitting a linear equation to observed data.\n",
    "\n",
    "Good: \n",
    "* Is great for seeing how changes to a single input variable affect one output variable. \n",
    "* It is commonly used for prediction (rather than categorization), which is exactly what we want.\n",
    "* Works well for small datasets because relationships are esaily interpretable.\n",
    "\n",
    "Bad:\n",
    "* If the data does not make linear relationships then this algorithm is very bad.\n",
    "\n",
    "### **KNN** ###\n",
    "K-Nearest Neighbors(KNN) is an algorithm that predicts the outcome for a new data point based on the outcomes of the 'k' nearest points in the training data.\n",
    "\n",
    "Good:\n",
    "* Can be used for both prediction as well as categorization.\n",
    "\n",
    "Bad:\n",
    "* For large datasets with a high number of columns, this will be extremely computationally expensive, and may experience the \"curse of dimensionality\".\n",
    "* Sensitive to noise and outliers. Performance varies based on the choice of 'k', which means we will have to tune.\n",
    "\n",
    "### **Decision Trees** ###\n",
    "Decision Trees split data into branches based on feature values, ultimately assigning an outcome to each branch.\n",
    "\n",
    "Good:\n",
    "* Suitable for capturing non-linear relationships in small datasets, while maintining interpretability.\n",
    "\n",
    "Bad:\n",
    "* Prone to overfitting. Performs much better with regularization or ensemble methods, so you may as well use a Random Forest. \n",
    "\n",
    "### **Random Forests** ###\n",
    "Random Forests create an ensemble of decision trees, each trained on a random subset of the data to improve predictive accuracy.\n",
    "\n",
    "Good:\n",
    "* Reduces overfitting.\n",
    "* Handles non-linear patterns.\n",
    "\n",
    "Bad:\n",
    "* With small datasets, overfitting may be even worse. \n",
    "* Much more computationally expensive than simple decision trees as it creates an ensemble. \n",
    "\n",
    "### **Gradient Boosting** ###\n",
    "Gradient Boosting builds an ensemble of weak models (typically decision trees), where each subsequent model corrects errors of its predecessor.\n",
    "\n",
    "Good\n",
    "* Capable of capturing complex relationships, even on small datasets.\n",
    "\n",
    "Bad\n",
    "* Computationally expensive.\n",
    "* Requires fine tuning, but with small datasets that is hard to do. \n",
    "\n",
    "## Conclusion ##\n",
    "Our goal is to predict distance given lap data. Seeing as Elysia's lap data contains 208 rows and 9 columns (after having dropped useless columns), **Linear Regression** is a suitable choice for the base model. As we are just aiming to display a simple model on the website for now, we want something that is lightweight, simple, and easy to interpret, and Linear Regression matches all these criteria. \n",
    "\n",
    "We do not want to risk running into KNN and dimensionality issues. Decision Trees, Random Forests, and Gradient Boosting either take too much computation, or we may even lack the data to fine tune these models. \n",
    "\n",
    "### References ###\n",
    "(1) Most Popular Machine Learning Algorithms: https://www.coursera.org/articles/machine-learning-algorithms \\\n",
    "(2) KNN and the Curse of Dimensionality: https://www.geeksforgeeks.org/k-nearest-neighbors-and-curse-of-dimensionality/\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules and Check the Data Size ##\n",
    "Should be 208x9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>secondsdifference</th>\n",
       "      <th>totalpowerin</th>\n",
       "      <th>totalpowerout</th>\n",
       "      <th>netpowerout</th>\n",
       "      <th>distance</th>\n",
       "      <th>amphours</th>\n",
       "      <th>batterysecondsremaining</th>\n",
       "      <th>averagespeed</th>\n",
       "      <th>averagepackCurrent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3264851</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>69.800003</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88892470</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>126.800003</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>95990970</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>98.699997</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>205000</td>\n",
       "      <td>793.532455</td>\n",
       "      <td>1803.385827</td>\n",
       "      <td>1009.853372</td>\n",
       "      <td>2.814824</td>\n",
       "      <td>97.800003</td>\n",
       "      <td>22425</td>\n",
       "      <td>49.584229</td>\n",
       "      <td>15.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3517768</td>\n",
       "      <td>798.704841</td>\n",
       "      <td>685.739611</td>\n",
       "      <td>-112.965230</td>\n",
       "      <td>4.118535</td>\n",
       "      <td>95.300003</td>\n",
       "      <td>56521</td>\n",
       "      <td>24.939703</td>\n",
       "      <td>6.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>325001</td>\n",
       "      <td>475.938651</td>\n",
       "      <td>1666.858482</td>\n",
       "      <td>1190.919832</td>\n",
       "      <td>3.963359</td>\n",
       "      <td>2.100000</td>\n",
       "      <td>406</td>\n",
       "      <td>43.894226</td>\n",
       "      <td>18.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>2494999</td>\n",
       "      <td>700.176665</td>\n",
       "      <td>566.324558</td>\n",
       "      <td>-133.852107</td>\n",
       "      <td>0.181906</td>\n",
       "      <td>4.700000</td>\n",
       "      <td>100215</td>\n",
       "      <td>1.106129</td>\n",
       "      <td>-5.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>343500</td>\n",
       "      <td>615.194700</td>\n",
       "      <td>1680.524756</td>\n",
       "      <td>1065.330056</td>\n",
       "      <td>3.963687</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>611</td>\n",
       "      <td>41.645220</td>\n",
       "      <td>17.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>330500</td>\n",
       "      <td>624.008188</td>\n",
       "      <td>1543.656620</td>\n",
       "      <td>919.648432</td>\n",
       "      <td>3.966359</td>\n",
       "      <td>1.400000</td>\n",
       "      <td>300</td>\n",
       "      <td>43.276732</td>\n",
       "      <td>16.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>-1123290860</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>73.800003</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>208 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     secondsdifference  totalpowerin  totalpowerout  netpowerout  distance  \\\n",
       "0              3264851      0.000000       0.000000     0.000000  0.000000   \n",
       "1             88892470      0.000000       0.000000     0.000000  0.000000   \n",
       "2             95990970      0.000000       0.000000     0.000000  0.000000   \n",
       "3               205000    793.532455    1803.385827  1009.853372  2.814824   \n",
       "4              3517768    798.704841     685.739611  -112.965230  4.118535   \n",
       "..                 ...           ...            ...          ...       ...   \n",
       "203             325001    475.938651    1666.858482  1190.919832  3.963359   \n",
       "204            2494999    700.176665     566.324558  -133.852107  0.181906   \n",
       "205             343500    615.194700    1680.524756  1065.330056  3.963687   \n",
       "206             330500    624.008188    1543.656620   919.648432  3.966359   \n",
       "207        -1123290860      0.000000       0.000000     0.000000  0.000000   \n",
       "\n",
       "       amphours  batterysecondsremaining  averagespeed  averagepackCurrent  \n",
       "0     69.800003                       -1      0.000000                 NaN  \n",
       "1    126.800003                       -1      0.000000                 NaN  \n",
       "2     98.699997                       -1      0.000000                 NaN  \n",
       "3     97.800003                    22425     49.584229               15.70  \n",
       "4     95.300003                    56521     24.939703                6.07  \n",
       "..          ...                      ...           ...                 ...  \n",
       "203    2.100000                      406     43.894226               18.62  \n",
       "204    4.700000                   100215      1.106129               -5.78  \n",
       "205    3.000000                      611     41.645220               17.67  \n",
       "206    1.400000                      300     43.276732               16.79  \n",
       "207   73.800003                       -1      0.000000                 NaN  \n",
       "\n",
       "[208 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "packetTrainingDataPath=\"../training_data/Elysia.Laps.feather\"\n",
    "df = pd.read_feather(packetTrainingDataPath)\n",
    "df = df.drop(\n",
    "        columns=[\n",
    "            \"msgType\",\n",
    "            \"_id.$oid\",\n",
    "            \"averagepackCurrent.$numberDouble\",\n",
    "            \"timestamp.$numberLong\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (root mean squared error) is: 36.22178511618355\n",
      "MAE (mean absolute error) is: 6.9643536149352085\n",
      "Minimum distance: -227.01091494750978\n",
      "Maximum distance: 77.62573291015624\n",
      "Average distance: 3.219106722576028\n",
      "25th percentile of distance: 3.962990234375\n",
      "50th percentile (median) of distance: 3.983119140625\n",
      "75th percentile of distance: 4.00380712890625\n"
     ]
    }
   ],
   "source": [
    "#we need the averagepackCurrent data to be numeric instead of {\"$numberDouble\": \"NaN\"}, setting errors='coerce' sets them to numerical NaN\n",
    "df['averagepackCurrent'] = pd.to_numeric(df['averagepackCurrent'], errors='coerce')\n",
    "\n",
    "#drop the 4 rows with null values\n",
    "df = df.dropna(subset=['distance', 'averagepackCurrent', 'averagespeed'])\n",
    "\n",
    "#seperate distance from the other features\n",
    "X = df[['secondsdifference', 'totalpowerin', 'totalpowerout', 'netpowerout', 'amphours', \n",
    "        'averagepackCurrent', 'batterysecondsremaining', 'averagespeed']]\n",
    "y = df['distance']\n",
    "\n",
    "#split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=69420)\n",
    "\n",
    "#train the baseline model with linear regression\n",
    "linear_regression_model = LinearRegression()\n",
    "linear_regression_model.fit(X_train, y_train)\n",
    "\n",
    "#get predictions on test data\n",
    "y_pred = linear_regression_model.predict(X_test)\n",
    "\n",
    "#evaluate using RMSE and MAE\n",
    "RMSE = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"RMSE (root mean squared error) is: {RMSE}\")\n",
    "print(f\"MAE (mean absolute error) is: {MAE}\")\n",
    "\n",
    "def print_distance_statistics():\n",
    "        min_distance = df['distance'].min()\n",
    "        max_distance = df['distance'].max()\n",
    "        average_distance = df['distance'].mean()\n",
    "        percentile_25 = np.percentile(df['distance'], 25)\n",
    "        percentile_50 = np.percentile(df['distance'], 50)\n",
    "        percentile_75 = np.percentile(df['distance'], 75)\n",
    "\n",
    "        print(\"Minimum distance:\", min_distance)\n",
    "        print(\"Maximum distance:\", max_distance)\n",
    "        print(\"Average distance:\", average_distance)\n",
    "        print(f\"25th percentile of distance: {percentile_25}\")\n",
    "        print(f\"50th percentile (median) of distance: {percentile_50}\")\n",
    "        print(f\"75th percentile of distance: {percentile_75}\")\n",
    "\n",
    "print_distance_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretation of Performance #\n",
    "Given such little spread in the IQR (middle 50% of distance values), but RMSE of 36.22 and MAE of 6.96, it is clear that this baseline model is very inaccurate. There should be very few predictions of distance which differ from a value of 4. This suggests that there could be large outliers, and so lets try this again, but cleaning outliers and negative distance values (it does not make sense for distance to be negative.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE (root mean squared error) after removing outliers is: 1.711367174002224\n",
      "MAE (mean absolute error) after removing outliers is: 0.9110000515477485\n",
      "Minimum distance: 0.0162265625\n",
      "Maximum distance: 8.745853515625\n",
      "Average distance: 4.063722782813743\n",
      "25th percentile of distance: 3.963570068359375\n",
      "50th percentile (median) of distance: 3.9834296875\n",
      "75th percentile of distance: 4.0039501953125\n"
     ]
    }
   ],
   "source": [
    "# Remove outliers from the data\n",
    "#to do this, first define a threshold for outliers (3 standard deviations which contains 99.7% of data)\n",
    "threshold = 3 * np.std(df['distance'])\n",
    "\n",
    "#remove the outliers\n",
    "df = df[(df['distance'] >= -threshold) & (df['distance'] <= threshold)]\n",
    "#remove negative distance values\n",
    "df = df[df['distance'] >= 0]\n",
    "#seperate distance from the other features\n",
    "X = df[['secondsdifference', 'totalpowerin', 'totalpowerout', 'netpowerout', 'amphours', \n",
    "    'averagepackCurrent', 'batterysecondsremaining', 'averagespeed']]\n",
    "y = df['distance']\n",
    "\n",
    "#split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=69420)\n",
    "\n",
    "#retrain with linear regression\n",
    "linear_regression_model.fit(X_train, y_train)\n",
    "\n",
    "#get predictions on test data\n",
    "y_pred = linear_regression_model.predict(X_test)\n",
    "\n",
    "#re-evaluate RMSE and MAE\n",
    "RMSE = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "MAE = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"RMSE (root mean squared error) after removing outliers is: {RMSE}\")\n",
    "print(f\"MAE (mean absolute error) after removing outliers is: {MAE}\")\n",
    "\n",
    "print_distance_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Re-evaluation of Performance #\n",
    "It is clear to see that the accuracy has improved significantly, as RMSE has gone from 36.22 to 1.7, and MAE hsa gone from 6.96 to 0.9. This means the difference in predictions of distance from actual values are much smaller. However, there are other simple tests we can try to improve the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated RMSE: 1.8882794269274583\n",
      "Cross-validated MAE: 0.8029010278510377\n",
      "Minimum distance: 0.0162265625\n",
      "Maximum distance: 8.745853515625\n",
      "Average distance: 4.063722782813743\n",
      "25th percentile of distance: 3.963570068359375\n",
      "50th percentile (median) of distance: 3.9834296875\n",
      "75th percentile of distance: 4.0039501953125\n"
     ]
    }
   ],
   "source": [
    "#initialize the KFold cross validator\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=69420)\n",
    "\n",
    "#lists to store RMSE and MAE for each fold (each subset of data)\n",
    "rmse_list = []\n",
    "mae_list = []\n",
    "\n",
    "#initialize a linear regression model\n",
    "linear_regression_model = LinearRegression()\n",
    "\n",
    "#cross-validation\n",
    "for train_index, test_index in kf.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    #train model\n",
    "    linear_regression_model.fit(X_train, y_train)\n",
    "    \n",
    "    #make predictions\n",
    "    y_pred = linear_regression_model.predict(X_test)\n",
    "    \n",
    "    #calculate RMSE and MAE for the fold\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse_list.append(rmse)\n",
    "    mae_list.append(mae)\n",
    "\n",
    "#get the average RMSE and MAE across all folds\n",
    "average_rmse = np.mean(rmse_list)\n",
    "average_mae = np.mean(mae_list)\n",
    "\n",
    "print(f\"Cross-validated RMSE: {average_rmse}\")\n",
    "print(f\"Cross-validated MAE: {average_mae}\")\n",
    "\n",
    "print_distance_statistics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Evaluation of Performance #\n",
    "Cross validation seems to worsen RMSE from 1.7 to 1.88, but improves MAE from 0.9 to 0.8. In general, low RMSE means the model is better at estimating outliers, whereas low MAE means the model is more robust and can predict around the median of data. Seeing as we will clean and remove the outliers anyway, we want to focus on predicting the majority of input data. So, we are prioritising a low MAE. Thus, this model using Cross-validation is the best so far. \n",
    "\n",
    "Since the majority of our data is between 3.9-4.0 and MAE is 0.8, it means predictions are normally 20% off from the real distance value. That is, in the real world, this model can be used to predict the distance of a lap given input features such as power, `amphours`, `averagepackCurrent`, `batterysecondsremaining`, and `averagespeed`. With a MAE of 0.8, the model's predictions are typically within 0.8 units of the actual distance. This prediction can be useful for making data driven decisions when optimizing lap performance, planning energy consumption, and looking at improving overall efficiency. By reducing the MAE, we ensure that the model is able to predict the majority of the input data, which has many applications in optimizing different metrics of the car during race."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
