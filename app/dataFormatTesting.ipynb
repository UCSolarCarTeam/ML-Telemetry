{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of different storage options for Helios packet data #\n",
    "##  JSON vs CSV vs Parquet vs Feather ##\n",
    "JSON is the most common format for storing data in Helios. However, it is not the most efficient format for storing large amounts of data. In this notebook, we will compare the performance of different storage options for Helios packet data. We will compare the performance of CSV, JSON, Parquet, and Feather formats. \n",
    "\n",
    "### JSON ###\n",
    "JSON is a text-based format that is easy to read and write. It is the main format that the Telemetry website uses to receive incoming packeData from the vehicle. However, JSON is not the most efficient format for storing large amounts of data. JSON files are larger than other formats, and reading and writing JSON files can be slow.\n",
    "\n",
    "### CSV ###\n",
    "CSV is a text-based format that is easy to read and write. It is a common format for storing tabular data. CSV files are smaller than JSON files, but they are still larger than other formats. Reading and writing CSV files can be slow but still much faster than JSON files.\n",
    "\n",
    "### Parquet ###\n",
    "Parquet is a columnar storage format that is optimized for reading and writing large amounts of data. Parquet files are smaller than JSON and CSV files, and reading and writing Parquet files is much faster than JSON and CSV files. Parquet files are also self-describing, meaning that they contain metadata that describes the structure of the data. Parquet, while not as common as JSON or csv, is still supported by many machine learning and data analysis libraries.\n",
    "\n",
    "### Feather ###\n",
    "Feather is a binary columnar storage format that is optimized for reading and writing large amounts of data. Feather files are smaller than JSON and CSV files, and reading and writing Feather files is much faster than JSON, and CSV files. Feather files, similar to Parquet files are also self-describing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Benchmark Comparisons ##\n",
    "We will compare the performance of reading and writing JSON, CSV, Parquet, and Feather files using the Elysia packet data. We will measure the time it takes to read and write each format and compare the file sizes of each format.\n",
    "\n",
    "This benchmark comparison uses the python time library to measure the time it takes to read and write each format. While this is not the most accurate way to measure performance, it provides a quick and easy way to estimate the performance of each format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import time\n",
    "import os\n",
    "packetTrainingDataPath=\"./training_data/Elysia.Packets.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON read time: 53.84395909309387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\justi\\AppData\\Local\\Temp\\ipykernel_12936\\3073500106.py:8: DtypeWarning: Columns (124,125,126,127) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('output.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV read time: 35.935688972473145\n",
      "Parquet read time: 3.2210075855255127\n",
      "Feather read time: 1.4769089221954346\n"
     ]
    }
   ],
   "source": [
    "# Reading JSON\n",
    "start_time = time.time()\n",
    "df = pd.read_json(packetTrainingDataPath)\n",
    "print(\"JSON read time:\", time.time() - start_time)\n",
    "\n",
    "# Reading CSV\n",
    "start_time = time.time()\n",
    "df = pd.read_csv('output.csv')\n",
    "print(\"CSV read time:\", time.time() - start_time)\n",
    "\n",
    "# Reading Parquet\n",
    "start_time = time.time()\n",
    "df = pq.read_table('output.parquet').to_pandas()\n",
    "print(\"Parquet read time:\", time.time() - start_time)\n",
    "\n",
    "# Reading feather\n",
    "start_time = time.time()\n",
    "df = pd.read_feather('output.feather')\n",
    "print(\"Feather read time:\", time.time() - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON write time: 38.231643199920654\n",
      "CSV write time: 23.684067010879517\n",
      "Parquet write time: 2.1993298530578613\n",
      "Feather write time: 1.1889092922210693\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Writing JSON\n",
    "start_time = time.time()\n",
    "df.to_json('output.json', orient='records', lines=True)\n",
    "print(\"JSON write time:\", time.time() - start_time)\n",
    "\n",
    "# Writing CSV\n",
    "start_time = time.time()\n",
    "df.to_csv('output.csv', index=False)\n",
    "print(\"CSV write time:\", time.time() - start_time)\n",
    "\n",
    "# Wrting Parquet\n",
    "start_time = time.time()\n",
    "table = pa.Table.from_pandas(df)\n",
    "pq.write_table(table, 'output.parquet')\n",
    "print(\"Parquet write time:\", time.time() - start_time)\n",
    "\n",
    "# Writing feather\n",
    "start_time = time.time()\n",
    "df.to_feather('output.feather')\n",
    "print(\"Feather write time:\", time.time() - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original JSON size (Megabytes): 2871.9577856063843\n",
      "JSON size (Megabytes): 2860.488380432129\n",
      "CSV size (Megabytes): 1283.9465894699097\n",
      "Parquet size (Megabytes): 149.94902420043945\n",
      "Feather size (Megabytes): 145.51709175109863\n"
     ]
    }
   ],
   "source": [
    "# Size of original JSON in megabytes\n",
    "print(\"Original JSON size (Megabytes):\", os.path.getsize(packetTrainingDataPath) / 1024 / 1024)\n",
    "\n",
    "# Size of JSON\n",
    "print(\"JSON size (Megabytes):\", os.path.getsize('output.json')/ 1024 / 1024)\n",
    "\n",
    "# Size of CSV\n",
    "print(\"CSV size (Megabytes):\", os.path.getsize('output.csv')/ 1024 / 1024)\n",
    "\n",
    "# Size of Parquet\n",
    "print(\"Parquet size (Megabytes):\", os.path.getsize('output.parquet')/ 1024 / 1024)\n",
    "\n",
    "# Size of feather\n",
    "print(\"Feather size (Megabytes):\", os.path.getsize('output.feather')/ 1024 / 1024)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion ##\n",
    "Based on the benchmark comparison,  Feather is the best storage options for Helios packet data, with Parquet as a viable option as well. Parquet and Feather files are smaller than JSON and CSV files, and reading and writing Parquet and Feather files is much faster than JSON and CSV files. \n",
    "\n",
    "## References ##\n",
    "1. https://arrow.apache.org/docs/python/feather.html\n",
    "2. https://arrow.apache.org/docs/python/parquet.html\n",
    "3. https://ursalabs.org/blog/2020-feather-v2/#:~:text=Parquet%20is%20fast%20to%20read,is%20even%20faster%20to%20read.&text=In%20the%20case%20of%20Feather,converting%20to%20R%20or%20pandas. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
